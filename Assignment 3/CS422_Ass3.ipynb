{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f41e7fe-85fd-4661-9e7f-363d42e5a08e",
   "metadata": {},
   "source": [
    "# Practicum Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae24177b-59ad-43ce-a067-98891505291c",
   "metadata": {},
   "source": [
    "### 2.1 Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f196fbca-5c3e-41ef-9648-380c94875002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataframe info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 398 entries, 0 to 397\n",
      "Data columns (total 9 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   mpg           398 non-null    float64\n",
      " 1   cylinders     398 non-null    int64  \n",
      " 2   displacement  398 non-null    float64\n",
      " 3   horsepower    392 non-null    float64\n",
      " 4   weight        398 non-null    float64\n",
      " 5   acceleration  398 non-null    float64\n",
      " 6   model_year    398 non-null    int64  \n",
      " 7   origin        398 non-null    int64  \n",
      " 8   car_name      398 non-null    object \n",
      "dtypes: float64(5), int64(3), object(1)\n",
      "memory usage: 28.1+ KB\n",
      "None\n",
      "\n",
      "Summary statistics:\n",
      "              mpg   cylinders  displacement  horsepower       weight  \\\n",
      "count  398.000000  398.000000    398.000000  392.000000   398.000000   \n",
      "mean    23.514573    5.454774    193.425879  104.469388  2970.424623   \n",
      "std      7.815984    1.701004    104.269838   38.491160   846.841774   \n",
      "min      9.000000    3.000000     68.000000   46.000000  1613.000000   \n",
      "25%     17.500000    4.000000    104.250000   75.000000  2223.750000   \n",
      "50%     23.000000    4.000000    148.500000   93.500000  2803.500000   \n",
      "75%     29.000000    8.000000    262.000000  126.000000  3608.000000   \n",
      "max     46.600000    8.000000    455.000000  230.000000  5140.000000   \n",
      "\n",
      "       acceleration  model_year      origin  \n",
      "count    398.000000  398.000000  398.000000  \n",
      "mean      15.568090   76.010050    1.572864  \n",
      "std        2.757689    3.697627    0.802055  \n",
      "min        8.000000   70.000000    1.000000  \n",
      "25%       13.825000   73.000000    1.000000  \n",
      "50%       15.500000   76.000000    1.000000  \n",
      "75%       17.175000   79.000000    2.000000  \n",
      "max       24.800000   82.000000    3.000000  \n",
      "\n",
      "Missing values per column:\n",
      "mpg             0\n",
      "cylinders       0\n",
      "displacement    0\n",
      "horsepower      6\n",
      "weight          0\n",
      "acceleration    0\n",
      "model_year      0\n",
      "origin          0\n",
      "car_name        0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model_year</th>\n",
       "      <th>origin</th>\n",
       "      <th>car_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>chevrolet chevelle malibu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>buick skylark 320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>plymouth satellite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>amc rebel sst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>ford torino</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  \\\n",
       "0  18.0          8         307.0       130.0  3504.0          12.0   \n",
       "1  15.0          8         350.0       165.0  3693.0          11.5   \n",
       "2  18.0          8         318.0       150.0  3436.0          11.0   \n",
       "3  16.0          8         304.0       150.0  3433.0          12.0   \n",
       "4  17.0          8         302.0       140.0  3449.0          10.5   \n",
       "\n",
       "   model_year  origin                   car_name  \n",
       "0          70       1  chevrolet chevelle malibu  \n",
       "1          70       1          buick skylark 320  \n",
       "2          70       1         plymouth satellite  \n",
       "3          70       1              amc rebel sst  \n",
       "4          70       1                ford torino  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing Step 1: Loading the auto-mpg.csv dataset from the local directory using pandas read_csv\n",
    "import pandas as pd  \n",
    "\n",
    "# Load the dataset with appropriate column names based on the provided variable information\n",
    "column_names = [\n",
    "    \"mpg\",          # Target variable (continuous)\n",
    "    \"cylinders\",    # Feature (integer)\n",
    "    \"displacement\", # Feature (continuous)\n",
    "    \"horsepower\",   # Feature (continuous, has missing values)\n",
    "    \"weight\",       # Feature (continuous)\n",
    "    \"acceleration\", # Feature (continuous)\n",
    "    \"model_year\",   # Feature (integer)\n",
    "    \"origin\",       # Feature (integer, multi-valued discrete)\n",
    "    \"car_name\"      # ID (categorical string)\n",
    "]\n",
    "\n",
    "# Load the dataset with whitespace separator and specify '?' as missing value\n",
    "df = pd.read_csv('auto-mpg.data', names=column_names, sep=r'\\s+', na_values='?')\n",
    "\n",
    "# Show basic info about data types and non-null counts\n",
    "print(\"\\nDataframe info:\")\n",
    "print(df.info())\n",
    "\n",
    "# Display summary statistics for continuous and integer features\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Show number of missing values per column to understand data quality\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Display first few rows to confirm data loading\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b8319d9-91cb-4eec-b88c-b213a8398f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected continuous features:\n",
      "    mpg  displacement  horsepower  weight  acceleration\n",
      "0  18.0         307.0       130.0  3504.0          12.0\n",
      "1  15.0         350.0       165.0  3693.0          11.5\n",
      "2  18.0         318.0       150.0  3436.0          11.0\n",
      "3  16.0         304.0       150.0  3433.0          12.0\n",
      "4  17.0         302.0       140.0  3449.0          10.5\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Select Continuous Features\n",
    "# - From the full dataset, we isolate only the continuous fields for analysis and modeling\n",
    "# - These include mpg (target), displacement, horsepower, weight, and acceleration\n",
    "# - We create a new DataFrame X that contains only these continuous variables\n",
    "\n",
    "# Define the list of continuous feature column names\n",
    "continuous_features = ['mpg', 'displacement', 'horsepower', 'weight', 'acceleration']\n",
    "\n",
    "# Select only the continuous columns into a new DataFrame\n",
    "X = df[continuous_features].copy()\n",
    "\n",
    "# Display the first few rows of the selected continuous features\n",
    "print(\"Selected continuous features:\")\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80e35b4b-891b-4ae2-93d7-183afd3c32ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after mean imputation:\n",
      "mpg             0\n",
      "displacement    0\n",
      "horsepower      0\n",
      "weight          0\n",
      "acceleration    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Impute Missing Values\n",
    "# - The 'horsepower' column contains missing values represented as NaN\n",
    "# - We use mean imputation to fill in missing values for each continuous column\n",
    "# - This step ensures the dataset is complete and suitable for further processing\n",
    "\n",
    "# Replace missing values in each column with the column's mean\n",
    "X.fillna(X.mean(), inplace=True)\n",
    "\n",
    "# Verify that there are no missing values remaining\n",
    "print(\"Missing values after mean imputation:\")\n",
    "print(X.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf75a2d9-4964-410e-945a-f25d2d0e95e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized continuous features:\n",
      "        mpg  displacement  horsepower    weight  acceleration\n",
      "0 -0.706439      1.090604    0.669196  0.630870     -1.295498\n",
      "1 -1.090751      1.503514    1.586599  0.854333     -1.477038\n",
      "2 -0.706439      1.196232    1.193426  0.550470     -1.658577\n",
      "3 -0.962647      1.061796    1.193426  0.546923     -1.295498\n",
      "4 -0.834543      1.042591    0.931311  0.565841     -1.840117\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Standardize the Features\n",
    "# - While standardization was not explicitly required in the instructions, we apply it here because we are using Euclidean distance for clustering.\n",
    "# - Without standardization, features with larger scales (like 'weight' or 'displacement') could dominate distance calculations and distort clustering results.\n",
    "# - Standardizing ensures all continuous features contribute equally.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize and apply the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# (Optional) Convert back to DataFrame for easier interpretation\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=continuous_features)\n",
    "\n",
    "# Display first few rows of the standardized features\n",
    "print(\"Standardized continuous features:\")\n",
    "print(X_scaled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad798993-7502-4cec-b66b-c810464c41b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster counts:\n",
      "cluster\n",
      "0    297\n",
      "1     97\n",
      "2      4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Perform Hierarchical Clustering\n",
    "# - We use AgglomerativeClustering from sklearn to cluster the standardized continuous features.\n",
    "# - Parameters are chosen as per the assignment instructions:\n",
    "#     • n_clusters = 3 → create 3 clusters\n",
    "#     • linkage = 'average' → average distance between clusters\n",
    "#     • metric = 'euclidean' → standard distance measure (replaces deprecated 'affinity')\n",
    "# - We use default values for the rest (e.g., no distance_threshold) to enforce a shallow clustering tree.\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Initialize and apply the Agglomerative Clustering algorithm\n",
    "clustering = AgglomerativeClustering(\n",
    "    n_clusters=3,\n",
    "    linkage='average',\n",
    "    metric='euclidean'\n",
    ")\n",
    "\n",
    "# Fit the model and predict cluster labels for each row in the dataset\n",
    "cluster_labels = clustering.fit_predict(X_scaled)\n",
    "\n",
    "# Add the cluster labels to the original DataFrame for further analysis\n",
    "df['cluster'] = cluster_labels\n",
    "\n",
    "# Display the number of data points in each cluster\n",
    "print(\"Cluster counts:\")\n",
    "print(df['cluster'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cef30999-afab-4115-857c-1da156a8b240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample of data with cluster labels:\n",
      "      mpg  displacement  horsepower  weight  acceleration  cluster\n",
      "10   15.0         383.0       170.0  3563.0          10.0        1\n",
      "261  18.1         258.0       120.0  3410.0          15.1        0\n",
      "354  34.5         100.0         NaN  2320.0          15.8        0\n",
      "277  16.2         163.0       133.0  3410.0          15.8        0\n",
      "17   21.0         200.0        85.0  2587.0          16.0        0\n",
      "232  16.0         351.0       149.0  4335.0          14.5        1\n",
      "258  20.6         231.0       105.0  3380.0          15.8        0\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Assign Cluster Labels\n",
    "# - Fit the AgglomerativeClustering model on the standardized continuous features\n",
    "# - Predict cluster labels for each data point\n",
    "# - Add these cluster labels as a new column in the original DataFrame 'df' for further analysis\n",
    "\n",
    "# Note: If you already did fit_predict in Step 5, you can just assign labels here again\n",
    "\n",
    "# Fit model and predict cluster labels\n",
    "cluster_labels = clustering.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to the original DataFrame\n",
    "df['cluster'] = cluster_labels\n",
    "\n",
    "# Verify by displaying random sample of 7 rows with cluster labels to get a quick look\n",
    "print(\"Random sample of data with cluster labels:\")\n",
    "print(df[['mpg', 'displacement', 'horsepower', 'weight', 'acceleration', 'cluster']].sample(7, random_state=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b23adb7-ee92-40f7-a596-f8e9405d1ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Statistics (Mean and Variance):\n",
      "\n",
      "Cluster 0:\n",
      "  Mpg         : Mean = 26.18, Variance = 41.30\n",
      "  Displacement: Mean = 144.30, Variance = 3511.49\n",
      "  Horsepower  : Mean = 86.12, Variance = 294.55\n",
      "  Weight      : Mean = 2598.41, Variance = 299118.71\n",
      "  Acceleration: Mean = 16.43, Variance = 4.88\n",
      "\n",
      "Cluster 1:\n",
      "  Mpg         : Mean = 14.53, Variance = 4.77\n",
      "  Displacement: Mean = 348.02, Variance = 2089.50\n",
      "  Horsepower  : Mean = 161.80, Variance = 674.08\n",
      "  Weight      : Mean = 4143.97, Variance = 193847.05\n",
      "  Acceleration: Mean = 12.64, Variance = 3.19\n",
      "\n",
      "Cluster 2:\n",
      "  Mpg         : Mean = 43.70, Variance = 0.30\n",
      "  Displacement: Mean = 91.75, Variance = 12.25\n",
      "  Horsepower  : Mean = 49.00, Variance = 4.00\n",
      "  Weight      : Mean = 2133.75, Variance = 21672.92\n",
      "  Acceleration: Mean = 22.88, Variance = 2.31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Compute Cluster Statistics with improved formatting\n",
    "\n",
    "# Group by cluster and calculate mean and variance\n",
    "cluster_stats = df.groupby('cluster')[continuous_features].agg(['mean', 'var'])\n",
    "\n",
    "# Flatten MultiIndex columns for cleaner display\n",
    "cluster_stats.columns = ['_'.join(col).strip() for col in cluster_stats.columns.values]\n",
    "\n",
    "# Display cluster statistics sorted by cluster label\n",
    "print(\"Cluster Statistics (Mean and Variance):\\n\")\n",
    "for cluster in sorted(df['cluster'].unique()):\n",
    "    print(f\"Cluster {cluster}:\")\n",
    "    cluster_data = cluster_stats.loc[cluster]\n",
    "    for feature in continuous_features:\n",
    "        mean_val = cluster_data[f\"{feature}_mean\"]\n",
    "        var_val = cluster_data[f\"{feature}_var\"]\n",
    "        print(f\"  {feature.capitalize():12}: Mean = {mean_val:.2f}, Variance = {var_val:.2f}\")\n",
    "    print()  # Blank line for readability between clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85fbd0b8-21f9-4a19-8f14-17fe671029c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origin Class Statistics (Mean and Variance):\n",
      "\n",
      "Origin 1:\n",
      "  Mpg         : Mean = 20.08, Variance = 41.00\n",
      "  Displacement: Mean = 245.90, Variance = 9702.61\n",
      "  Horsepower  : Mean = 119.05, Variance = 1591.83\n",
      "  Weight      : Mean = 3361.93, Variance = 631695.13\n",
      "  Acceleration: Mean = 15.03, Variance = 7.57\n",
      "\n",
      "Origin 2:\n",
      "  Mpg         : Mean = 27.89, Variance = 45.21\n",
      "  Displacement: Mean = 109.14, Variance = 509.95\n",
      "  Horsepower  : Mean = 80.56, Variance = 406.34\n",
      "  Weight      : Mean = 2423.30, Variance = 240142.33\n",
      "  Acceleration: Mean = 16.79, Variance = 9.28\n",
      "\n",
      "Origin 3:\n",
      "  Mpg         : Mean = 30.45, Variance = 37.09\n",
      "  Displacement: Mean = 102.71, Variance = 535.47\n",
      "  Horsepower  : Mean = 79.84, Variance = 317.52\n",
      "  Weight      : Mean = 2221.23, Variance = 102718.49\n",
      "  Acceleration: Mean = 16.17, Variance = 3.82\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Compute Origin Class Statistics\n",
    "# - Group the original DataFrame by 'origin' (1=USA, 2=Europe, 3=Japan)\n",
    "# - Calculate the mean and variance of continuous features within each origin group\n",
    "# - This helps compare how clusters correspond to known origin classes\n",
    "\n",
    "# Group by 'origin' and calculate mean and variance for continuous features\n",
    "origin_stats = df.groupby('origin')[continuous_features].agg(['mean', 'var'])\n",
    "\n",
    "# Display neatly formatted origin statistics\n",
    "print(\"Origin Class Statistics (Mean and Variance):\")\n",
    "for origin in origin_stats.index:\n",
    "    print(f\"\\nOrigin {origin}:\")\n",
    "    for feature in continuous_features:\n",
    "        mean_val = origin_stats.loc[origin, (feature, 'mean')]\n",
    "        var_val = origin_stats.loc[origin, (feature, 'var')]\n",
    "        print(f\"  {feature.capitalize():<12}: Mean = {mean_val:.2f}, Variance = {var_val:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a46442d9-e699-421b-b396-e0708c7df8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crosstab: Number of cars from each origin in each cluster\n",
      "\n",
      "         USA  Europe  Japan\n",
      "cluster                    \n",
      "0        152      66     79\n",
      "1         97       0      0\n",
      "2          0       4      0\n",
      "\n",
      "Interpretation:\n",
      "\n",
      "Cluster 0:\n",
      "  USA cars: 152\n",
      "  Europe cars: 66\n",
      "  Japan cars: 79\n",
      "  --> Dominant origin: USA with 152 cars (51.18% of this cluster)\n",
      "\n",
      "Cluster 1:\n",
      "  USA cars: 97\n",
      "  Europe cars: 0\n",
      "  Japan cars: 0\n",
      "  --> Dominant origin: USA with 97 cars (100.00% of this cluster)\n",
      "\n",
      "Cluster 2:\n",
      "  USA cars: 0\n",
      "  Europe cars: 4\n",
      "  Japan cars: 0\n",
      "  --> Dominant origin: Europe with 4 cars (100.00% of this cluster)\n",
      "\n",
      "Summary:\n",
      "- Cluster 1 predominantly groups cars from USA.\n",
      "- Cluster 2 mostly contains European cars but is very small in size.\n",
      "- Cluster 0 contains a mixture of cars from all origins (USA, Europe, Japan).\n",
      "- This shows partial alignment between clusters and known origin classes, but clusters do not perfectly separate the data by origin.\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Compare Clusters with Origin Classes\n",
    "\n",
    "# Mapping origin numbers to meaningful names\n",
    "origin_names = {1: 'USA', 2: 'Europe', 3: 'Japan'}\n",
    "\n",
    "# Create crosstab: counts of cars from each origin within each cluster\n",
    "crosstab_named = pd.crosstab(df['cluster'], df['origin'])\n",
    "# Rename columns from numbers to origin names\n",
    "crosstab_named.columns = [origin_names.get(col, f\"Origin {col}\") for col in crosstab_named.columns]\n",
    "\n",
    "# Display the crosstab table with origin names\n",
    "print(\"Crosstab: Number of cars from each origin in each cluster\\n\")\n",
    "print(crosstab_named)\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "\n",
    "# For each cluster, print count of cars from each origin and find dominant origin\n",
    "for cluster in crosstab_named.index:\n",
    "    print(f\"\\nCluster {cluster}:\")\n",
    "    for origin in crosstab_named.columns:\n",
    "        count = crosstab_named.loc[cluster, origin]\n",
    "        print(f\"  {origin} cars: {count}\")\n",
    "    \n",
    "    # Determine dominant origin in the cluster\n",
    "    dominant_origin = crosstab_named.loc[cluster].idxmax()\n",
    "    dominant_count = crosstab_named.loc[cluster].max()\n",
    "    total_in_cluster = crosstab_named.loc[cluster].sum()\n",
    "    percent = (dominant_count / total_in_cluster) * 100 if total_in_cluster > 0 else 0\n",
    "    \n",
    "    print(f\"  --> Dominant origin: {dominant_origin} with {dominant_count} cars \"\n",
    "          f\"({percent:.2f}% of this cluster)\")\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(\"- Cluster 1 predominantly groups cars from USA.\")\n",
    "print(\"- Cluster 2 mostly contains European cars but is very small in size.\")\n",
    "print(\"- Cluster 0 contains a mixture of cars from all origins (USA, Europe, Japan).\")\n",
    "print(\"- This shows partial alignment between clusters and known origin classes, but clusters do not perfectly separate the data by origin.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc7af8e-73d5-49bd-a8cf-640f49984b81",
   "metadata": {},
   "source": [
    "# Step 10: Evaluation of Relationship Between Cluster Assignments and Origin Classes\n",
    "\n",
    "---\n",
    "\n",
    "## Origin Class Statistics (Mean and Variance)\n",
    "\n",
    "| Feature      | Origin 1 (USA)       | Origin 2 (Europe)    | Origin 3 (Japan)     |\n",
    "|--------------|----------------------|---------------------|---------------------|\n",
    "| **MPG**         | Mean = 20.08, Var = 41.00  | Mean = 27.89, Var = 45.21 | Mean = 30.45, Var = 37.09 |\n",
    "| **Displacement**| Mean = 245.90, Var = 9702.61 | Mean = 109.14, Var = 509.95 | Mean = 102.71, Var = 535.47 |\n",
    "| **Horsepower**  | Mean = 119.05, Var = 1591.83 | Mean = 80.56, Var = 406.34  | Mean = 79.84, Var = 317.52  |\n",
    "| **Weight**      | Mean = 3361.93, Var = 631695.13 | Mean = 2423.30, Var = 240142.33 | Mean = 2221.23, Var = 102718.49 |\n",
    "| **Acceleration**| Mean = 15.03, Var = 7.57   | Mean = 16.79, Var = 9.28   | Mean = 16.17, Var = 3.82   |\n",
    "\n",
    "---\n",
    "\n",
    "## Cluster Statistics (Mean and Variance)\n",
    "\n",
    "| Feature      | Cluster 0           | Cluster 1          | Cluster 2          |\n",
    "|--------------|---------------------|--------------------|--------------------|\n",
    "| **MPG**         | Mean = 26.18, Var = 41.30  | Mean = 14.53, Var = 4.77  | Mean = 43.70, Var = 0.30  |\n",
    "| **Displacement**| Mean = 144.30, Var = 3511.49 | Mean = 348.02, Var = 2089.50 | Mean = 91.75, Var = 12.25  |\n",
    "| **Horsepower**  | Mean = 86.12, Var = 294.55  | Mean = 161.80, Var = 674.08 | Mean = 49.00, Var = 4.00   |\n",
    "| **Weight**      | Mean = 2598.41, Var = 299118.71 | Mean = 4143.97, Var = 193847.05 | Mean = 2133.75, Var = 21672.92 |\n",
    "| **Acceleration**| Mean = 16.43, Var = 4.88   | Mean = 12.64, Var = 3.19   | Mean = 22.88, Var = 2.31   |\n",
    "\n",
    "---\n",
    "\n",
    "## Crosstab: Distribution of Origin Classes Within Each Cluster\n",
    "\n",
    "| Cluster | USA Cars | Europe Cars | Japan Cars | Dominant Origin (Count, %)        |\n",
    "|---------|----------|-------------|------------|----------------------------------|\n",
    "| 0       | 152      | 66          | 79         | USA (152 cars, 51.18% of cluster)|\n",
    "| 1       | 97       | 0           | 0          | USA (97 cars, 100% of cluster)   |\n",
    "| 2       | 0        | 4           | 0          | Europe (4 cars, 100% of cluster) |\n",
    "\n",
    "---\n",
    "\n",
    "## Analysis and Interpretation\n",
    "\n",
    "- **Cluster 1** clearly corresponds to **Origin 1 (USA)**:\n",
    "  - Exclusively USA vehicles (100% dominance).\n",
    "  - Matches well with Origin 1’s higher displacement, horsepower, and weight but lower MPG.\n",
    "  \n",
    "- **Cluster 2** aligns mostly with **Origin 2 (Europe)**:\n",
    "  - Small cluster but 100% European vehicles.\n",
    "  - Characterized by the highest MPG and acceleration, and lowest displacement and horsepower.\n",
    "  \n",
    "- **Cluster 0** is a **mixed cluster**, containing cars from all three origins:\n",
    "  - USA cars dominate but only about half (51%).\n",
    "  - Shows intermediate values in features, blending characteristics across origins.\n",
    "\n",
    "- **Variance values** indicate that:\n",
    "  - Cluster 0 has relatively high variance, supporting the mixed-origin composition.\n",
    "  - Clusters 1 and 2 show tighter, more homogeneous groupings.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion: Is There a Clear Relationship?\n",
    "\n",
    "- There **is a partial relationship** between clusters and origin classes.\n",
    "- Clusters 1 and 2 **effectively separate** USA and European cars respectively.\n",
    "- However, Cluster 0’s mixture of origins indicates **imperfect separation**.\n",
    "- The overlap likely results from shared vehicle characteristics across origins and limitations of hierarchical clustering with chosen parameters.\n",
    "- To improve clarity, consider:\n",
    "  - Trying other clustering algorithms,\n",
    "  - Using dimensionality reduction (e.g., PCA),\n",
    "  - Selecting different feature sets.\n",
    "\n",
    "---\n",
    "\n",
    "This comprehensive comparison reveals that while clustering captures some meaningful groupings aligned with origin, it does **not perfectly classify vehicles by origin**.\n",
    "\n",
    "\n",
    "\n",
    "### Citations:\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "- https://www.kaggle.com/code/johnybhiduri/auto-mpg-clustering\n",
    "- https://github.com/jaredbest/machine-learning-with-python/blob/master/Labs/10_Hierarchical_Clustering.ipynb\n",
    "- https://www.kaggle.com/code/datarohitingole/data-clustering-using-kmeans-and-detailed-eda\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfa04cc-1389-45d8-9ba0-0169013d52b4",
   "metadata": {},
   "source": [
    "### 2.2 Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78531634-769e-4beb-80b4-d665afd4da29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/var/folders/yw/xmm4_b6105ngdx58blhgx4y40000gn/T/ipykernel_13739/419020585.py:12: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Load Boston dataset manually from original source and create a DataFrame\n",
    "\n",
    "# Note: The Boston housing dataset has been removed from scikit-learn since version 1.2\n",
    "# due to ethical concerns regarding a variable ('B') related to racial bias.\n",
    "# Because of this, sklearn discourages its use and removed the loader.\n",
    "# Therefore, we load the dataset manually from the original source URL for educational purposes only.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]\n",
    "\n",
    "feature_names = [\n",
    "    'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE',\n",
    "    'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT'\n",
    "]\n",
    "\n",
    "df_boston = pd.DataFrame(data, columns=feature_names)\n",
    "df_boston['MEDV'] = target\n",
    "\n",
    "df_boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daee4286-c42d-47fa-abe1-2713080ab7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.419782</td>\n",
       "      <td>0.284830</td>\n",
       "      <td>-1.287909</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.144217</td>\n",
       "      <td>0.413672</td>\n",
       "      <td>-0.120013</td>\n",
       "      <td>0.140214</td>\n",
       "      <td>-0.982843</td>\n",
       "      <td>-0.666608</td>\n",
       "      <td>-1.459000</td>\n",
       "      <td>0.441052</td>\n",
       "      <td>-1.075562</td>\n",
       "      <td>0.159686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.417339</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-0.593381</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.740262</td>\n",
       "      <td>0.194274</td>\n",
       "      <td>0.367166</td>\n",
       "      <td>0.557160</td>\n",
       "      <td>-0.867883</td>\n",
       "      <td>-0.987329</td>\n",
       "      <td>-0.303094</td>\n",
       "      <td>0.441052</td>\n",
       "      <td>-0.492439</td>\n",
       "      <td>-0.101524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.417342</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-0.593381</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.740262</td>\n",
       "      <td>1.282714</td>\n",
       "      <td>-0.265812</td>\n",
       "      <td>0.557160</td>\n",
       "      <td>-0.867883</td>\n",
       "      <td>-0.987329</td>\n",
       "      <td>-0.303094</td>\n",
       "      <td>0.396427</td>\n",
       "      <td>-1.208727</td>\n",
       "      <td>1.324247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.416750</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-1.306878</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.835284</td>\n",
       "      <td>1.016303</td>\n",
       "      <td>-0.809889</td>\n",
       "      <td>1.077737</td>\n",
       "      <td>-0.752922</td>\n",
       "      <td>-1.106115</td>\n",
       "      <td>0.113032</td>\n",
       "      <td>0.416163</td>\n",
       "      <td>-1.361517</td>\n",
       "      <td>1.182758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.412482</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-1.306878</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.835284</td>\n",
       "      <td>1.228577</td>\n",
       "      <td>-0.511180</td>\n",
       "      <td>1.077737</td>\n",
       "      <td>-0.752922</td>\n",
       "      <td>-1.106115</td>\n",
       "      <td>0.113032</td>\n",
       "      <td>0.441052</td>\n",
       "      <td>-1.026501</td>\n",
       "      <td>1.487503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
       "0 -0.419782  0.284830 -1.287909 -0.272599 -0.144217  0.413672 -0.120013   \n",
       "1 -0.417339 -0.487722 -0.593381 -0.272599 -0.740262  0.194274  0.367166   \n",
       "2 -0.417342 -0.487722 -0.593381 -0.272599 -0.740262  1.282714 -0.265812   \n",
       "3 -0.416750 -0.487722 -1.306878 -0.272599 -0.835284  1.016303 -0.809889   \n",
       "4 -0.412482 -0.487722 -1.306878 -0.272599 -0.835284  1.228577 -0.511180   \n",
       "\n",
       "        DIS       RAD       TAX   PTRATIO         B     LSTAT      MEDV  \n",
       "0  0.140214 -0.982843 -0.666608 -1.459000  0.441052 -1.075562  0.159686  \n",
       "1  0.557160 -0.867883 -0.987329 -0.303094  0.441052 -0.492439 -0.101524  \n",
       "2  0.557160 -0.867883 -0.987329 -0.303094  0.396427 -1.208727  1.324247  \n",
       "3  1.077737 -0.752922 -1.106115  0.113032  0.416163 -1.361517  1.182758  \n",
       "4  1.077737 -0.752922 -1.106115  0.113032  0.441052 -1.026501  1.487503  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Scale the Data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# We scale the features to have mean = 0 and standard deviation = 1\n",
    "# This is important for K-Means because it is a distance-based algorithm\n",
    "# and unscaled features (e.g., TAX vs. CRIM) can dominate the clustering outcome.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df_boston)\n",
    "\n",
    "# Convert back to a DataFrame for easier inspection and compatibility\n",
    "df_scaled = pd.DataFrame(scaled_data, columns=df_boston.columns)\n",
    "\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8df4aab0-1a7b-43f1-a9c1-2a5704b0c620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Cluster distribution for k = 2:\n",
      "  Cluster 0: 329 samples\n",
      "  Cluster 1: 177 samples\n",
      "\n",
      "🔹 Cluster distribution for k = 3:\n",
      "  Cluster 0: 156 samples\n",
      "  Cluster 1: 209 samples\n",
      "  Cluster 2: 141 samples\n",
      "\n",
      "🔹 Cluster distribution for k = 4:\n",
      "  Cluster 0: 201 samples\n",
      "  Cluster 1: 76 samples\n",
      "  Cluster 2: 103 samples\n",
      "  Cluster 3: 126 samples\n",
      "\n",
      "🔹 Cluster distribution for k = 5:\n",
      "  Cluster 0: 231 samples\n",
      "  Cluster 1: 119 samples\n",
      "  Cluster 2: 83 samples\n",
      "  Cluster 3: 34 samples\n",
      "  Cluster 4: 39 samples\n",
      "\n",
      "🔹 Cluster distribution for k = 6:\n",
      "  Cluster 0: 42 samples\n",
      "  Cluster 1: 193 samples\n",
      "  Cluster 2: 41 samples\n",
      "  Cluster 3: 79 samples\n",
      "  Cluster 4: 34 samples\n",
      "  Cluster 5: 117 samples\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Perform K-Means Clustering for k = 2 to 6\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Define range of k values to try\n",
    "k_values = range(2, 7)\n",
    "\n",
    "# Initialize dictionaries to store results\n",
    "kmeans_models = {}      # Stores KMeans model for each k\n",
    "cluster_labels = {}     # Stores cluster labels for each k\n",
    "\n",
    "# Loop over k values and fit KMeans to the scaled data\n",
    "for k in k_values:\n",
    "    # Fit KMeans on scaled data\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(scaled_data)  # 'scaled_data' is the output from StandardScaler\n",
    "    kmeans_models[k] = kmeans\n",
    "    cluster_labels[k] = kmeans.labels_\n",
    "\n",
    "    # We're saving each model and its assigned cluster labels for comparison in the next steps\n",
    "\n",
    "# Optional: Show how many samples are in each cluster for each k\n",
    "for k in k_values:\n",
    "    print(f\"\\n🔹 Cluster distribution for k = {k}:\")\n",
    "    labels = cluster_labels[k]\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    for cluster_id, count in zip(unique, counts):\n",
    "        print(f\"  Cluster {cluster_id}: {count} samples\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7312f0b3-1320-4da0-9333-4c69a2e47bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Scores for different k values:\n",
      "\n",
      "  k = 2: Silhouette Score = 0.3501\n",
      "  k = 3: Silhouette Score = 0.2370\n",
      "  k = 4: Silhouette Score = 0.2589\n",
      "  k = 5: Silhouette Score = 0.2707\n",
      "  k = 6: Silhouette Score = 0.2780\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Calculate Silhouette Scores\n",
    "\n",
    "# For each value of k, we evaluate how well the clusters are formed using Silhouette Score.\n",
    "# The Silhouette Score ranges from -1 to 1: higher values mean better-defined clusters.\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_scores = {}  # To store the scores for each k\n",
    "\n",
    "# Loop through each fitted KMeans model\n",
    "for k in k_values:\n",
    "    labels = cluster_labels[k]  # Get labels for current k\n",
    "    score = silhouette_score(scaled_data, labels)  # Compute silhouette score on scaled data\n",
    "    silhouette_scores[k] = score  # Store it\n",
    "\n",
    "# Display all scores\n",
    "print(\"Silhouette Scores for different k values:\\n\")\n",
    "for k, score in silhouette_scores.items():\n",
    "    print(f\"  k = {k}: Silhouette Score = {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb2280-5ed9-43bc-b38e-a435edf074e6",
   "metadata": {},
   "source": [
    "### Step 5: Determine the Optimal Number of Clusters (k)\n",
    "\n",
    "To select the best number of clusters for K-Means clustering, we compared **Silhouette Scores** for values of **k ranging from 2 to 6**. Silhouette Score is a measure of how well each data point fits within its cluster and how distinct each cluster is from others.\n",
    "\n",
    "#### Silhouette Score Results:\n",
    "\n",
    "| Number of Clusters (k) | Silhouette Score |\n",
    "|------------------------|------------------|\n",
    "| 2                      | **0.3501** ✅     |\n",
    "| 3                      | 0.2370           |\n",
    "| 4                      | 0.2589           |\n",
    "| 5                      | 0.2707           |\n",
    "| 6                      | 0.2780           |\n",
    "\n",
    "#### Interpretation:\n",
    "- The **highest Silhouette Score** is observed for **k = 2**.\n",
    "- This suggests that when the data is grouped into **2 clusters**, it achieves the **best cohesion (within-cluster similarity)** and **separation (between-cluster dissimilarity)**.\n",
    "\n",
    "---\n",
    "\n",
    "###  Conclusion:\n",
    "- The **optimal number of clusters (k)** is **2**, as it provides the most meaningful and well-formed partitioning of the standardized data based on the Silhouette Score.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "371fc050-38f6-46ee-8cec-57e8ea51d4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster assignments for first 10 data points: [0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "Centroid coordinates for each cluster:\n",
      "[[-0.38980122  0.26239167 -0.61529402  0.00291182 -0.58291594  0.24491263\n",
      "  -0.43358416  0.45449141 -0.58345172 -0.62972689 -0.29466201  0.32860027\n",
      "  -0.45349747  0.35364132]\n",
      " [ 0.72454577 -0.48772236  1.14368211 -0.00541237  1.08349913 -0.45523307\n",
      "   0.80592762 -0.84478912  1.08449501  1.1705093   0.54770508 -0.61078808\n",
      "   0.84294162 -0.6573333 ]]\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Perform K-Means Clustering with the optimal number of clusters (k=2)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Assuming 'scaled_data' is the scaled numpy array from Step 2 (if using DataFrame, convert accordingly)\n",
    "# If you used a DataFrame named 'scaled_df', make sure it's defined; else, use 'scaled_data' here.\n",
    "\n",
    "optimal_k = 2\n",
    "\n",
    "# Initialize KMeans with optimal k\n",
    "kmeans_optimal = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "\n",
    "# Fit KMeans on the scaled data array (make sure variable name matches your scaled data)\n",
    "kmeans_optimal.fit(scaled_data)  # scaled_data from step 2, numpy array or DataFrame values\n",
    "\n",
    "# Cluster labels for each data point\n",
    "cluster_labels_optimal = kmeans_optimal.labels_\n",
    "\n",
    "# Centroid coordinates of each cluster\n",
    "cluster_centroids = kmeans_optimal.cluster_centers_\n",
    "\n",
    "# Print first 10 cluster assignments and centroid coordinates\n",
    "print(f\"Cluster assignments for first 10 data points: {cluster_labels_optimal[:10]}\")\n",
    "print(\"\\nCentroid coordinates for each cluster:\")\n",
    "print(cluster_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0a3d2fea-fa01-44f4-b02e-e19b52110242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Values for Each Feature in Each Cluster (Optimal k=2):\n",
      "\n",
      "Cluster 0:\n",
      "  CRIM      : Mean = 0.26\n",
      "  ZN        : Mean = 17.48\n",
      "  INDUS     : Mean = 6.92\n",
      "  CHAS      : Mean = 0.07\n",
      "  NOX       : Mean = 0.49\n",
      "  RM        : Mean = 6.46\n",
      "  AGE       : Mean = 56.38\n",
      "  DIS       : Mean = 4.75\n",
      "  RAD       : Mean = 4.47\n",
      "  TAX       : Mean = 302.21\n",
      "  PTRATIO   : Mean = 17.82\n",
      "  B         : Mean = 386.64\n",
      "  LSTAT     : Mean = 9.42\n",
      "  MEDV      : Mean = 25.78\n",
      "\n",
      "Cluster 1:\n",
      "  CRIM      : Mean = 9.84\n",
      "  ZN        : Mean = 0.00\n",
      "  INDUS     : Mean = 18.98\n",
      "  CHAS      : Mean = 0.07\n",
      "  NOX       : Mean = 0.68\n",
      "  RM        : Mean = 5.97\n",
      "  AGE       : Mean = 91.24\n",
      "  DIS       : Mean = 2.02\n",
      "  RAD       : Mean = 18.98\n",
      "  TAX       : Mean = 605.32\n",
      "  PTRATIO   : Mean = 19.64\n",
      "  B         : Mean = 300.97\n",
      "  LSTAT     : Mean = 18.67\n",
      "  MEDV      : Mean = 16.49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Calculate Mean Values for Each Cluster\n",
    "# Explanation: Using the optimal K-Means clustering (k=2) from Step 6, we assign cluster labels\n",
    "# to the original (unscaled) DataFrame and compute the mean value for each feature within each cluster.\n",
    "# This uses the original feature values (not scaled) to interpret the cluster characteristics in their natural scale.\n",
    "\n",
    "# Add cluster labels to the original DataFrame\n",
    "df_boston['cluster'] = cluster_labels_optimal\n",
    "\n",
    "# Group by cluster and calculate the mean for each feature\n",
    "# Note: We include all features (excluding 'MEDV' if it's considered the target, but including it here as it's part of the dataset)\n",
    "cluster_means = df_boston.groupby('cluster').mean()\n",
    "\n",
    "# Display the mean values for each cluster, formatted for readability\n",
    "print(\"\\nMean Values for Each Feature in Each Cluster (Optimal k=2):\\n\")\n",
    "for cluster in cluster_means.index:\n",
    "    print(f\"Cluster {cluster}:\")\n",
    "    for feature in cluster_means.columns:\n",
    "        mean_val = cluster_means.loc[cluster, feature]\n",
    "        print(f\"  {feature:<10}: Mean = {mean_val:.2f}\")\n",
    "    print()  # Blank line for readability between clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8854c61-2094-4b5e-be64-d8c6cae469c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison in Scaled Space: Cluster Means vs. Centroids (k=2)\n",
      "\n",
      "Cluster 0:\n",
      "  Feature       Scaled Mean  Centroid   Difference\n",
      "  ----------------------------------------\n",
      "  CRIM            -0.3898    -0.3898     0.0000\n",
      "  ZN               0.2624     0.2624    -0.0000\n",
      "  INDUS           -0.6153    -0.6153    -0.0000\n",
      "  CHAS             0.0029     0.0029     0.0000\n",
      "  NOX             -0.5829    -0.5829    -0.0000\n",
      "  RM               0.2449     0.2449     0.0000\n",
      "  AGE             -0.4336    -0.4336    -0.0000\n",
      "  DIS              0.4545     0.4545    -0.0000\n",
      "  RAD             -0.5835    -0.5835     0.0000\n",
      "  TAX             -0.6297    -0.6297     0.0000\n",
      "  PTRATIO         -0.2947    -0.2947     0.0000\n",
      "  B                0.3286     0.3286     0.0000\n",
      "  LSTAT           -0.4535    -0.4535    -0.0000\n",
      "  MEDV             0.3536     0.3536     0.0000\n",
      "\n",
      "Cluster 1:\n",
      "  Feature       Scaled Mean  Centroid   Difference\n",
      "  ----------------------------------------\n",
      "  CRIM             0.7245     0.7245    -0.0000\n",
      "  ZN              -0.4877    -0.4877    -0.0000\n",
      "  INDUS            1.1437     1.1437    -0.0000\n",
      "  CHAS            -0.0054    -0.0054     0.0000\n",
      "  NOX              1.0835     1.0835    -0.0000\n",
      "  RM              -0.4552    -0.4552     0.0000\n",
      "  AGE              0.8059     0.8059    -0.0000\n",
      "  DIS             -0.8448    -0.8448     0.0000\n",
      "  RAD              1.0845     1.0845    -0.0000\n",
      "  TAX              1.1705     1.1705    -0.0000\n",
      "  PTRATIO          0.5477     0.5477    -0.0000\n",
      "  B               -0.6108    -0.6108     0.0000\n",
      "  LSTAT            0.8429     0.8429    -0.0000\n",
      "  MEDV            -0.6573    -0.6573    -0.0000\n",
      "\n",
      "\n",
      "Comparison in Unscaled Space: Cluster Means vs. Inverse-Transformed Centroids (k=2)\n",
      "\n",
      "Cluster 0:\n",
      "  Feature       Unscaled Mean  Centroid   Difference\n",
      "  ----------------------------------------\n",
      "  CRIM                 0.26       0.26       0.00\n",
      "  ZN                  17.48      17.48      -0.00\n",
      "  INDUS                6.92       6.92      -0.00\n",
      "  CHAS                 0.07       0.07       0.00\n",
      "  NOX                  0.49       0.49       0.00\n",
      "  RM                   6.46       6.46       0.00\n",
      "  AGE                 56.38      56.38       0.00\n",
      "  DIS                  4.75       4.75      -0.00\n",
      "  RAD                  4.47       4.47       0.00\n",
      "  TAX                302.21     302.21       0.00\n",
      "  PTRATIO             17.82      17.82       0.00\n",
      "  B                  386.64     386.64       0.00\n",
      "  LSTAT                9.42       9.42       0.00\n",
      "  MEDV                25.78      25.78      -0.00\n",
      "\n",
      "Cluster 1:\n",
      "  Feature       Unscaled Mean  Centroid   Difference\n",
      "  ----------------------------------------\n",
      "  CRIM                 9.84       9.84      -0.00\n",
      "  ZN                   0.00       0.00      -0.00\n",
      "  INDUS               18.98      18.98      -0.00\n",
      "  CHAS                 0.07       0.07       0.00\n",
      "  NOX                  0.68       0.68       0.00\n",
      "  RM                   5.97       5.97       0.00\n",
      "  AGE                 91.24      91.24       0.00\n",
      "  DIS                  2.02       2.02       0.00\n",
      "  RAD                 18.98      18.98      -0.00\n",
      "  TAX                605.32     605.32      -0.00\n",
      "  PTRATIO             19.64      19.64      -0.00\n",
      "  B                  300.97     300.97       0.00\n",
      "  LSTAT               18.67      18.67      -0.00\n",
      "  MEDV                16.49      16.49       0.00\n",
      "\n",
      "\n",
      "Analysis of Differences:\n",
      "- In K-Means, centroids represent the mean of each cluster in the feature space.\n",
      "- In the scaled space, cluster means and centroids should be almost identical (differences ~0).\n",
      "- In the unscaled space, minor differences may exist due to inverse-scaling and floating-point rounding.\n",
      "- These results validate that centroids are accurate representations of the clusters.\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Compare Cluster Means to Centroid Coordinates\n",
    "# Explanation: We compare the mean values of each feature in each cluster (from Step 7) to the K-Means\n",
    "# centroid coordinates for the optimal clustering (k=2). Since K-Means was performed on scaled data,\n",
    "# we compute scaled cluster means to compare directly with centroids. We also inverse-transform the\n",
    "# centroids to the original scale to compare with the unscaled cluster means from Step 7. Any differences\n",
    "# should be negligible, as K-Means centroids are the means of the data points in each cluster.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# If cluster labels or centroids are not already defined, re-run clustering\n",
    "try:\n",
    "    cluster_labels_optimal\n",
    "    cluster_centroids\n",
    "except NameError:\n",
    "    optimal_k = 2\n",
    "    kmeans_optimal = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "    # Drop 'cluster' column if it exists in scaled DataFrame before fitting\n",
    "    kmeans_optimal.fit(df_scaled.drop(columns='cluster', errors='ignore'))  \n",
    "    cluster_labels_optimal = kmeans_optimal.labels_\n",
    "    cluster_centroids = kmeans_optimal.cluster_centers_\n",
    "\n",
    "# Add cluster labels to the scaled DataFrame\n",
    "df_scaled['cluster'] = cluster_labels_optimal\n",
    "\n",
    "# Calculate scaled means: Group by cluster and compute mean for each feature\n",
    "cluster_means_scaled = df_scaled.groupby('cluster').mean()\n",
    "\n",
    "# Extract feature columns only (exclude 'cluster' and any target column if necessary)\n",
    "feature_columns = df_boston.columns.drop('cluster', errors='ignore')\n",
    "\n",
    "# Inverse-transform centroids to original scale using the trained scaler from Step 2\n",
    "centroids_unscaled = scaler.inverse_transform(cluster_centroids)\n",
    "\n",
    "# Create DataFrame for centroids with only feature columns\n",
    "centroids_unscaled_df = pd.DataFrame(centroids_unscaled, columns=feature_columns, index=range(centroids_unscaled.shape[0]))\n",
    "\n",
    "# Display scaled cluster means and centroids for direct comparison\n",
    "print(\"\\nComparison in Scaled Space: Cluster Means vs. Centroids (k=2)\\n\")\n",
    "for cluster in cluster_means_scaled.index:\n",
    "    print(f\"Cluster {cluster}:\")\n",
    "    print(\"  Feature       Scaled Mean  Centroid   Difference\")\n",
    "    print(\"  \" + \"-\"*40)\n",
    "    for feature in cluster_means_scaled.columns:\n",
    "        if feature != 'cluster':  # Skip cluster column\n",
    "            mean_val = cluster_means_scaled.loc[cluster, feature]\n",
    "            centroid_val = cluster_centroids[cluster][df_scaled.columns.get_loc(feature)]\n",
    "            difference = mean_val - centroid_val\n",
    "            print(f\"  {feature:<12} {mean_val:>10.4f} {centroid_val:>10.4f} {difference:>10.4f}\")\n",
    "    print()\n",
    "\n",
    "# Recompute cluster means on unscaled data\n",
    "df_boston['cluster'] = cluster_labels_optimal\n",
    "cluster_means_unscaled = df_boston.groupby('cluster').mean()\n",
    "\n",
    "# Display unscaled cluster means vs. inverse-transformed centroids\n",
    "print(\"\\nComparison in Unscaled Space: Cluster Means vs. Inverse-Transformed Centroids (k=2)\\n\")\n",
    "for cluster in cluster_means_unscaled.index:\n",
    "    print(f\"Cluster {cluster}:\")\n",
    "    print(\"  Feature       Unscaled Mean  Centroid   Difference\")\n",
    "    print(\"  \" + \"-\"*40)\n",
    "    for feature in cluster_means_unscaled.columns:\n",
    "        mean_val = cluster_means_unscaled.loc[cluster, feature]\n",
    "        centroid_val = centroids_unscaled_df.loc[cluster, feature]\n",
    "        difference = mean_val - centroid_val\n",
    "        print(f\"  {feature:<12} {mean_val:>12.2f} {centroid_val:>10.2f} {difference:>10.2f}\")\n",
    "    print()\n",
    "\n",
    "# Interpretation of results\n",
    "print(\"\\nAnalysis of Differences:\")\n",
    "print(\"- In K-Means, centroids represent the mean of each cluster in the feature space.\")\n",
    "print(\"- In the scaled space, cluster means and centroids should be almost identical (differences ~0).\")\n",
    "print(\"- In the unscaled space, minor differences may exist due to inverse-scaling and floating-point rounding.\")\n",
    "print(\"- These results validate that centroids are accurate representations of the clusters.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d300891-cebf-4989-b105-3de2ce687ba7",
   "metadata": {},
   "source": [
    "### Step 8: Detailed Comparison of Cluster Means and Centroid Coordinates with Numbers\n",
    "\n",
    "In this step, we **compare the cluster means** obtained from the original dataset with the **centroid coordinates** from the K-Means clustering model. Since the clustering was performed on **scaled data**, it is important to analyze the results both in the **scaled feature space** and after **inverse-scaling** back to the original feature values.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why This Comparison Matters\n",
    "\n",
    "- **K-Means centroids** are the **mean** of all data points assigned to each cluster in the feature space used for clustering.\n",
    "- Our K-Means model was applied on **standardized (scaled)** data, so the **centroids are in scaled coordinates**.\n",
    "- We compare:\n",
    "  - The **scaled cluster means** with the **centroids in scaled space**.\n",
    "  - The **original cluster means** (unscaled) with the **inverse-transformed centroids**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Findings: Scaled Space Comparison (k=2)\n",
    "\n",
    "| Cluster | Feature | Scaled Mean | Centroid   | Difference  |\n",
    "|---------|---------|-------------|------------|-------------|\n",
    "| 0       | CRIM    | -0.3898     | -0.3898    | 0.0000      |\n",
    "|         | ZN      | 0.2624      | 0.2624     | -0.0000     |\n",
    "|         | INDUS   | -0.6153     | -0.6153    | -0.0000     |\n",
    "|         | CHAS    | 0.0029      | 0.0029     | 0.0000      |\n",
    "|         | NOX     | -0.5829     | -0.5829    | -0.0000     |\n",
    "|         | RM      | 0.2449      | 0.2449     | 0.0000      |\n",
    "|         | AGE     | -0.4336     | -0.4336    | -0.0000     |\n",
    "|         | DIS     | 0.4545      | 0.4545     | -0.0000     |\n",
    "|         | RAD     | -0.5835     | -0.5835    | 0.0000      |\n",
    "|         | TAX     | -0.6297     | -0.6297    | 0.0000      |\n",
    "|         | PTRATIO | -0.2947     | -0.2947    | 0.0000      |\n",
    "|         | B       | 0.3286      | 0.3286     | 0.0000      |\n",
    "|         | LSTAT   | -0.4535     | -0.4535    | -0.0000     |\n",
    "|         | MEDV    | 0.3536      | 0.3536     | 0.0000      |\n",
    "\n",
    "| Cluster | Feature | Scaled Mean | Centroid   | Difference  |\n",
    "|---------|---------|-------------|------------|-------------|\n",
    "| 1       | CRIM    | 0.7245      | 0.7245     | -0.0000     |\n",
    "|         | ZN      | -0.4877     | -0.4877    | -0.0000     |\n",
    "|         | INDUS   | 1.1437      | 1.1437     | -0.0000     |\n",
    "|         | CHAS    | -0.0054     | -0.0054    | 0.0000      |\n",
    "|         | NOX     | 1.0835      | 1.0835     | -0.0000     |\n",
    "|         | RM      | -0.4552     | -0.4552    | 0.0000      |\n",
    "|         | AGE     | 0.8059      | 0.8059     | -0.0000     |\n",
    "|         | DIS     | -0.8448     | -0.8448    | 0.0000      |\n",
    "|         | RAD     | 1.0845      | 1.0845     | -0.0000     |\n",
    "|         | TAX     | 1.1705      | 1.1705     | -0.0000     |\n",
    "|         | PTRATIO | 0.5477      | 0.5477     | -0.0000     |\n",
    "|         | B       | -0.6108     | -0.6108    | 0.0000      |\n",
    "|         | LSTAT   | 0.8429      | 0.8429     | -0.0000     |\n",
    "|         | MEDV    | -0.6573     | -0.6573    | -0.0000     |\n",
    "\n",
    "- Differences are effectively **zero**, confirming **perfect match** between scaled cluster means and centroids.\n",
    "\n",
    "---\n",
    "\n",
    "#### Findings: Unscaled Space Comparison (k=2)\n",
    "\n",
    "| Cluster | Feature | Original Mean | Inverse-Scaled Centroid | Difference  |\n",
    "|---------|---------|---------------|------------------------|-------------|\n",
    "| 0       | CRIM    | 0.26          | 0.26                   | 0.00        |\n",
    "|         | ZN      | 17.48         | 17.48                  | -0.00       |\n",
    "|         | INDUS   | 6.92          | 6.92                   | -0.00       |\n",
    "|         | CHAS    | 0.07          | 0.07                   | 0.00        |\n",
    "|         | NOX     | 0.49          | 0.49                   | 0.00        |\n",
    "|         | RM      | 6.46          | 6.46                   | 0.00        |\n",
    "|         | AGE     | 56.38         | 56.38                  | 0.00        |\n",
    "|         | DIS     | 4.75          | 4.75                   | -0.00       |\n",
    "|         | RAD     | 4.47          | 4.47                   | 0.00        |\n",
    "|         | TAX     | 302.21        | 302.21                 | 0.00        |\n",
    "|         | PTRATIO | 17.82         | 17.82                  | 0.00        |\n",
    "|         | B       | 386.64        | 386.64                 | 0.00        |\n",
    "|         | LSTAT   | 9.42          | 9.42                   | 0.00        |\n",
    "|         | MEDV    | 25.78         | 25.78                  | -0.00       |\n",
    "\n",
    "| Cluster | Feature | Original Mean | Inverse-Scaled Centroid | Difference  |\n",
    "|---------|---------|---------------|------------------------|-------------|\n",
    "| 1       | CRIM    | 9.84          | 9.84                   | -0.00       |\n",
    "|         | ZN      | 0.00          | 0.00                   | -0.00       |\n",
    "|         | INDUS   | 18.98         | 18.98                  | -0.00       |\n",
    "|         | CHAS    | 0.07          | 0.07                   | 0.00        |\n",
    "|         | NOX     | 0.68          | 0.68                   | 0.00        |\n",
    "|         | RM      | 5.97          | 5.97                   | 0.00        |\n",
    "|         | AGE     | 91.24         | 91.24                  | 0.00        |\n",
    "|         | DIS     | 2.02          | 2.02                   | 0.00        |\n",
    "|         | RAD     | 18.98         | 18.98                  | -0.00       |\n",
    "|         | TAX     | 605.32        | 605.32                 | -0.00       |\n",
    "|         | PTRATIO | 19.64         | 19.64                  | -0.00       |\n",
    "|         | B       | 300.97        | 300.97                 | 0.00        |\n",
    "|         | LSTAT   | 18.67         | 18.67                  | -0.00       |\n",
    "|         | MEDV    | 16.49         | 16.49                  | 0.00        |\n",
    "\n",
    "- Differences are **negligible** and arise only due to **floating-point rounding** during scaling transformations.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary and Interpretation\n",
    "\n",
    "- **Centroids = Cluster Means** in scaled space, confirming the mathematical definition of K-Means.\n",
    "- After **inverse scaling**, centroids closely approximate the original feature means of clusters.\n",
    "- Minor differences are expected due to **numerical precision** and do not affect interpretation.\n",
    "- This confirms the **validity and accuracy** of our K-Means clustering results on the Boston dataset.\n",
    "\n",
    "---\n",
    "\n",
    "> **Conclusion:**  \n",
    "> The centroids are reliable representatives of cluster centers, allowing confident interpretation of cluster characteristics in both scaled and original feature spaces.\n",
    "\n",
    "## Citations:\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "- https://www.datacamp.com/tutorial/k-means-clustering-python\n",
    "- https://www.kaggle.com/code/nasimetemadi/clustring-of-customers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f86d5af-2927-4d00-a5b4-c3628b9ec907",
   "metadata": {},
   "source": [
    "### 2.3 Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6a4f697-eef3-4ebc-a830-fe45b5acd4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wine Dataset (First 5 Rows):\n",
      "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
      "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
      "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
      "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
      "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
      "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
      "\n",
      "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
      "0        3.06                  0.28             2.29             5.64  1.04   \n",
      "1        2.76                  0.26             1.28             4.38  1.05   \n",
      "2        3.24                  0.30             2.81             5.68  1.03   \n",
      "3        3.49                  0.24             2.18             7.80  0.86   \n",
      "4        2.69                  0.39             1.82             4.32  1.04   \n",
      "\n",
      "   od280/od315_of_diluted_wines  proline  class  \n",
      "0                          3.92   1065.0      0  \n",
      "1                          3.40   1050.0      0  \n",
      "2                          3.17   1185.0      0  \n",
      "3                          3.45   1480.0      0  \n",
      "4                          2.93    735.0      0  \n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the Wine Dataset\n",
    "# Explanation: We load the Wine dataset from sklearn.datasets and convert it into a Pandas DataFrame.\n",
    "# The dataset includes 13 features and a target variable (class labels: 0, 1, 2) representing wine cultivars.\n",
    "# We include the class labels in the DataFrame for later comparison with cluster assignments.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Load the Wine dataset\n",
    "wine = load_wine()\n",
    "\n",
    "# Create DataFrame with feature names and data\n",
    "df_wine = pd.DataFrame(data=wine.data, columns=wine.feature_names)\n",
    "\n",
    "# Add the actual class labels as a column\n",
    "df_wine['class'] = wine.target\n",
    "\n",
    "# Display the first few rows to verify loading\n",
    "print(\"Wine Dataset (First 5 Rows):\")\n",
    "print(df_wine.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddcc2bb9-b1e5-409e-b9ee-6288584f3897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scaled Wine Dataset (First 5 Rows):\n",
      "    alcohol  malic_acid       ash  alcalinity_of_ash  magnesium  \\\n",
      "0  1.518613   -0.562250  0.232053          -1.169593   1.913905   \n",
      "1  0.246290   -0.499413 -0.827996          -2.490847   0.018145   \n",
      "2  0.196879    0.021231  1.109334          -0.268738   0.088358   \n",
      "3  1.691550   -0.346811  0.487926          -0.809251   0.930918   \n",
      "4  0.295700    0.227694  1.840403           0.451946   1.281985   \n",
      "\n",
      "   total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\n",
      "0       0.808997    1.034819             -0.659563         1.224884   \n",
      "1       0.568648    0.733629             -0.820719        -0.544721   \n",
      "2       0.808997    1.215533             -0.498407         2.135968   \n",
      "3       2.491446    1.466525             -0.981875         1.032155   \n",
      "4       0.808997    0.663351              0.226796         0.401404   \n",
      "\n",
      "   color_intensity       hue  od280/od315_of_diluted_wines   proline  \n",
      "0         0.251717  0.362177                      1.847920  1.013009  \n",
      "1        -0.293321  0.406051                      1.113449  0.965242  \n",
      "2         0.269020  0.318304                      0.788587  1.395148  \n",
      "3         1.186068 -0.427544                      1.184071  2.334574  \n",
      "4        -0.319276  0.362177                      0.449601 -0.037874  \n"
     ]
    }
   ],
   "source": [
    "# Step 2: Scale the Data\n",
    "# Explanation: We standardize all features (excluding the class label) to have mean=0 and variance=1\n",
    "# using StandardScaler. This ensures fair distance calculations in K-Means clustering.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select only the feature columns (exclude 'class')\n",
    "features = wine.feature_names\n",
    "X = df_wine[features]\n",
    "\n",
    "# Apply standardization\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert scaled data back to a DataFrame for easier handling\n",
    "df_scaled = pd.DataFrame(X_scaled, columns=features)\n",
    "\n",
    "# Display the first few rows of scaled data\n",
    "print(\"\\nScaled Wine Dataset (First 5 Rows):\")\n",
    "print(df_scaled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08bd4c13-cb00-49c4-a52c-1ba853550303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster labels for first 10 data points: [2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Perform K-Means Clustering with k=3\n",
    "# Explanation: We apply K-Means clustering on the scaled data with k=3, as specified.\n",
    "# The random_state is set for reproducibility, and n_init=10 ensures multiple initializations for better results.\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Initialize and fit K-Means with k=3\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "kmeans.fit(X_scaled)\n",
    "\n",
    "# Store cluster labels\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Show first 10 predicted cluster labels\n",
    "print(\"Cluster labels for first 10 data points:\", cluster_labels[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "710373bf-3a9a-40a2-9674-6f6ccbe0dc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster Distribution (k=3):\n",
      "  Cluster 0: 65 samples\n",
      "  Cluster 1: 51 samples\n",
      "  Cluster 2: 62 samples\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Assign Cluster Labels\n",
    "# Explanation: We assign the predicted K-Means cluster labels to the original DataFrame.\n",
    "# This allows us to analyze how many data points fall into each cluster.\n",
    "\n",
    "# Add cluster labels to the DataFrame\n",
    "df_wine['cluster'] = cluster_labels\n",
    "\n",
    "# Display the number of data points in each cluster\n",
    "print(\"\\nCluster Distribution (k=3):\")\n",
    "for cluster_id, count in df_wine['cluster'].value_counts().sort_index().items():\n",
    "    print(f\"  Cluster {cluster_id}: {count} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7cda744c-cc39-405a-bd81-4abae5f78a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clustering Evaluation Metrics (k=3):\n",
      "  Homogeneity Score: 0.8788  (Higher is better, max=1.0)\n",
      "  Completeness Score: 0.8730  (Higher is better, max=1.0)\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Calculate Homogeneity and Completeness Scores\n",
    "# Explanation:\n",
    "# Homogeneity measures whether each cluster contains only members of a single class.\n",
    "# Completeness measures whether all members of a given class are assigned to the same cluster.\n",
    "# Both metrics range from 0 to 1, where 1 is perfect agreement.\n",
    "# We compare the true wine class labels with the K-Means cluster assignments to evaluate clustering quality.\n",
    "\n",
    "from sklearn.metrics import homogeneity_score, completeness_score\n",
    "\n",
    "# Calculate Homogeneity and Completeness scores\n",
    "homogeneity = homogeneity_score(df_wine['class'], cluster_labels)\n",
    "completeness = completeness_score(df_wine['class'], cluster_labels)\n",
    "\n",
    "# Display the evaluation results with clear formatting\n",
    "print(\"\\nClustering Evaluation Metrics (k=3):\")\n",
    "print(f\"  Homogeneity Score: {homogeneity:.4f}  (Higher is better, max=1.0)\")\n",
    "print(f\"  Completeness Score: {completeness:.4f}  (Higher is better, max=1.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16ad9719-3d8b-49dc-b32c-bbc0fee8113e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Interpretation of Clustering Metrics:\n",
      "\n",
      "Homogeneity Score: 0.8788\n",
      "  - Indicates the extent to which clusters contain only members of a single class.\n",
      "  - A score of 1.0 means perfect homogeneity: each cluster corresponds to exactly one class.\n",
      "  - Lower scores suggest that clusters include a mixture of different classes.\n",
      "\n",
      "Completeness Score: 0.8730\n",
      "  - Reflects whether all members of a given class are assigned to the same cluster.\n",
      "  - A score of 1.0 means perfect completeness: all points of a class are in one cluster.\n",
      "  - Lower scores indicate that a class is spread across multiple clusters.\n",
      "\n",
      "Summary:\n",
      "  - High homogeneity implies pure clusters without class mixing.\n",
      "  - High completeness implies minimal fragmentation of classes across clusters.\n",
      "  - Together, these metrics assess how well clustering captures the true underlying class structure.\n",
      "  - For our K-Means clustering (k=3), the scores suggest a strong alignment with actual wine classes.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Interpret Homogeneity and Completeness Metrics\n",
    "# Explanation:\n",
    "# - Homogeneity measures if each cluster contains only points from a single class.\n",
    "# - Completeness measures if all points of a class are assigned to the same cluster.\n",
    "# Both metrics together provide complementary insights on cluster quality.\n",
    "\n",
    "print(\"\\nInterpretation of Clustering Metrics:\\n\")\n",
    "\n",
    "print(f\"Homogeneity Score: {homogeneity:.4f}\")\n",
    "print(\"  - Indicates the extent to which clusters contain only members of a single class.\")\n",
    "print(\"  - A score of 1.0 means perfect homogeneity: each cluster corresponds to exactly one class.\")\n",
    "print(\"  - Lower scores suggest that clusters include a mixture of different classes.\\n\")\n",
    "\n",
    "print(f\"Completeness Score: {completeness:.4f}\")\n",
    "print(\"  - Reflects whether all members of a given class are assigned to the same cluster.\")\n",
    "print(\"  - A score of 1.0 means perfect completeness: all points of a class are in one cluster.\")\n",
    "print(\"  - Lower scores indicate that a class is spread across multiple clusters.\\n\")\n",
    "\n",
    "print(\"Summary:\")\n",
    "print(\"  - High homogeneity implies pure clusters without class mixing.\")\n",
    "print(\"  - High completeness implies minimal fragmentation of classes across clusters.\")\n",
    "print(\"  - Together, these metrics assess how well clustering captures the true underlying class structure.\")\n",
    "print(\"  - For our K-Means clustering (k=3), the scores suggest a strong alignment with actual wine classes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c998dcb9-f88a-40cf-b7fd-67b67f708ade",
   "metadata": {},
   "source": [
    "### Step 6: Detailed Interpretation of Clustering Metrics with Scores\n",
    "\n",
    "We evaluate the performance of the K-Means clustering (with **k = 3**) using the **Homogeneity** and **Completeness** metrics, which compare the cluster assignments to the true class labels of the Wine dataset.\n",
    "\n",
    "#### **Homogeneity Score: 0.8788**\n",
    "- This metric measures the extent to which **each cluster contains only members of a single class**.\n",
    "- A **perfect score of 1.0** means clusters are **completely pure** — every cluster corresponds to exactly one class.\n",
    "- Our score of **0.8788** indicates **high cluster purity**, meaning that approximately **87.88% of the clustering structure reflects true class homogeneity**.\n",
    "- The remaining **12.12%** difference suggests some **mixing of different classes within certain clusters**, but this is minimal.\n",
    "\n",
    "#### **Completeness Score: 0.8730**\n",
    "- This metric assesses whether **all samples from a given class are assigned to the same cluster**.\n",
    "- A **score of 1.0** means each class is **entirely grouped within a single cluster**, without fragmentation.\n",
    "- Our completeness score of **0.8730** implies that about **87.30% of the actual class membership is captured within individual clusters**.\n",
    "- The residual **12.70%** indicates a small amount of **class fragmentation across clusters**, where some classes are split into multiple clusters.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary and Insights**\n",
    "\n",
    "- Both **Homogeneity (0.8788)** and **Completeness (0.8730)** scores are **close to 1**, which is a strong indicator that the clustering model is doing a very good job at capturing the true underlying class structure.\n",
    "- **High homogeneity** confirms that clusters are largely **pure and contain samples from only one class**.\n",
    "- **High completeness** means that **most of the members of a single class are grouped together** in the same cluster.\n",
    "- The minor deviations from 1.0 (roughly 12%) reflect **some degree of overlap and class mixing**, which is expected in real-world data and unsupervised clustering.\n",
    "- Therefore, the K-Means clustering with **3 clusters** aligns well with the **actual wine classes**, validating its effectiveness in this context.\n",
    "\n",
    "## Citations:\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_score.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.metrics.completeness_score.html\n",
    "- https://www.kaggle.com/code/abdallahwagih/k-means-clustering\n",
    "- https://www.kaggle.com/code/digvijaysingh16/k-mean-clustering-for-wine-quality-data\n",
    "- https://www.kaggle.com/code/thedatageek/clustering-eda-analysis-clearly-explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a647987-346d-403c-a1bc-d41ad1229c71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616d9d18-ced4-470b-8471-5c4db4ec838a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
